seed: 420
env_id: HalfCheetah-v4
device: cpu
total_timesteps: 1_000_000
learning_starts: 10_000
run_path: ""
checkpoint: ""
log:
  wandb: False
  log_local: False
  save_models: False
sac:
  topk: 1
  n_experts: 8
  q_depth: 2
  policy_lr: 0.0003
  q_lr: 0.001
  buffer_size: 1_000_000
  gamma: 0.99  # discount factor
  tau: 0.005
  batch_size: 256
  policy_frequency: 2  # frequency of the policy training (delayed)
  target_network_frequency: 1
  alpha: 0.2  # temperature determining the relative importance of the entropy term against the reward
  alpha_auto: True
  nonlinear_actor: False
  nonlinear_actor_size: "full"
  aux_loss: "vision"
  aux_loss_weight: 0.01
